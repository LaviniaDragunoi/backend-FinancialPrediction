# Financial Prediction Backend Project

*This document combines the original project motivation with a detailed technical overview.*

---

## My Learning Journey

After reading "Head First Learn to Code" by Eric Freeman, I came up with the idea to create something to consolidate my new knowledge.

The idea started with the ambition to put to the test my new Python knowledge, adding some refreshing on Functional Programming and OOP implementation in a Python project, but more than that, using AI tools as guidance to help me implement with a main focus on FP principles.

This project involves developing the backend data management system for a basic financial application, allowing me to apply and solidify my entry-level Python skills. The primary goal is to create a robust and functional backend that manages financial data and prepares it for consumption by a separate mobile/web application.

I'll keep this doc to document all the steps that I'll follow while working on this project. This way, everything will be well-documented, as well as the structure followed and any suggestions that I receive from the AI tool that I'll use.

---

## Project Features

- **Data Ingestion:** Fetches time-series financial data (Open, High, Low, Close, Volume) from the [Alpha Vantage API](https://www.alphavantage.co/).
- **Data Processing:** Cleans the raw data and transforms it into sequences suitable for training a machine learning model.
- **API Endpoints:** Provides a simple REST API (using FastAPI) to trigger model training and retrieve predictions.
- **(In Progress) Model Training:** Uses the processed data to train a predictive model.

---

## Getting Started

Follow these instructions to set up and run the project on your local machine.

### 1. Prerequisites

- Python 3.9+
- An API key from [Alpha Vantage](https://www.alphavantage.co/support/#api-key) (it's free).

### 2. Installation & Setup

1.  **Clone the repository:**
    ```sh
    git clone <your-repository-url>
    cd financial_prediction
    ```

2.  **Create a virtual environment:**
    ```sh
    python3 -m venv .venv
    source .venv/bin/activate
    ```

3.  **Install the dependencies:**
    ```sh
    pip install -r requirements.txt
    ```

4.  **Set up your environment variables:**
    - Create a file named `.env` in the project root.
    - Add your Alpha Vantage API key to it:
      ```
      ALPHA_VANTAGE_API_KEY="YOUR_API_KEY_HERE"
      ```

### 3. Running the Application

Once the setup is complete, you can run the API server using Uvicorn:

```sh
uvicorn api.main:app --reload
```

The API will be available at `http://127.0.0.1:8000`.

---

## API Usage

You can interact with the API through its interactive documentation, which is automatically generated by FastAPI.

1.  **Open your browser** and navigate to `http://127.0.0.1:8000/docs`.

2.  **Trigger Model Training:**
    - Find the `POST /train` endpoint.
    - Click "Try it out".
    - You can modify the request body to specify a different stock ticker (e.g., `"MSFT"`).
    - Click "Execute". The API will fetch the data, process it, and return a confirmation message along with a summary of the cleaned data.

3.  **Get Predictions:**
    - Find the `GET /predict` endpoint.
    - Click "Try it out" and "Execute".
    - *Note: This endpoint is currently a placeholder and will be implemented after the model training logic is complete.*

---

## Project Architecture

The project is structured into distinct directories, each with a clear responsibility. This separation of concerns makes the codebase modular, scalable, and easy to maintain.

```
financial_prediction/
├── api/                # Handles all API-related logic (FastAPI).
│   └── main.py
├── data/               # Manages data sourcing and processing.
│   ├── connector.py
│   ├── extractor.py
│   └── processor.py
├── services/           # Orchestrates the core business logic.
│   └── data_pipeline_service.py
├── models/             # (Future) For model training logic and saved model files.
├── tests/              # Contains all tests for the application.
└── .env                # Stores secret keys and environment variables.
```

### Data Flow

The typical data flow for a training request is as follows:

1.  **API Layer (`api/main.py`):** A `POST` request is received at the `/train` endpoint.
2.  **Service Layer (`services/data_pipeline_service.py`):** The API calls the `run_data_pipeline` function, which acts as the central coordinator.
3.  **Data Layer (`data/`):**
    - **`connector.py`:** Establishes a connection with the Alpha Vantage API.
    - **`extractor.py`:** Fetches the data and converts it into a pandas DataFrame.
    - **`processor.py`:** Cleans and transforms the DataFrame into feature (X) and target (y) arrays.
4.  **Model Layer (`models/`):** (Future) The features and targets are used to train and save a predictive model.
5.  **API Layer (Response):** The API sends a JSON response back to the client, confirming the operation.

---

## Design Philosophy: The Synergy of OOP and FP

This project intentionally blends principles from both Object-Oriented Programming (OOP) and Functional Programming (FP) to create a robust and maintainable system.

### The Role of Object-Oriented Programming (OOP)

OOP is used to structure the application and manage its components (the "nouns" of the system).

- **Encapsulation:** The `MarketAPIConnector` class is a prime example. It bundles all the logic needed to communicate with the external API, hiding the complexity from the rest of the application.
- **Abstraction:** FastAPI's `app` object and Pydantic's `BaseModel` classes abstract away the complexities of the web server and data validation, providing simple interfaces to work with.

### The Role of Functional Programming (FP)

FP is used to handle the data processing and transformation logic (the "verbs" of the system).

- **Stateless Functions:** The functions in `processor.py` (`clean_data`, `create_sequences`) take data as input and produce new, transformed data as output without causing side effects. This makes them predictable and easy to test.
- **Function Composition:** The entire data pipeline is a chain of functions (`get_raw_data() -> clean_data() -> create_sequences()`), creating a clear and linear flow of data.

### Why This Hybrid Approach is Important

By using OOP to structure the components and FP to handle the data flow, the project gains a solid, scalable foundation that is also predictable, testable, and less prone to bugs.

---

## Development Log & Tools

- **AI:** Gemini - free version. The discussion that helped build this project can be followed here: https://gemini.google.com/share/b22fa762a413
